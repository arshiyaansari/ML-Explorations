{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-40VPC7MAGGB"
   },
   "source": [
    "# Assignment 4: Benchmarking Fashion-MNIST with Deep Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piFzh10hAGGE"
   },
   "source": [
    "### CS 4501 Machine Learning - Department of Computer Science - University of Virginia\n",
    "\"The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\" - **Zalando Research, Github Repo.**\"\n",
    "\n",
    "Fashion-MNIST is a dataset from the Zalando's article. Each example is a 28x28 grayscale image, associated with a label from 10 classes. They intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms.\n",
    "\n",
    "![Here's an example how the data looks (each class takes three-rows):](https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png)\n",
    "\n",
    "In this assignment, you will attempt to benchmark the Fashion-MNIST using Neural Networks. You must use it to train some neural networks on TensorFlow and predict the final output of 10 classes. For deliverables, you must write code in Python and submit this Jupyter Notebook file (.ipynb) to earn a total of 100 pts. You will gain points depending on how you perform in the following sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "469YvvIzAGGJ"
   },
   "outputs": [],
   "source": [
    "# You might want to use the following packages\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) #reduce annoying warning messages\n",
    "from functools import partial\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-PtpH4xAGGG"
   },
   "source": [
    "---\n",
    "## 1. PRE-PROCESSING THE DATA (10 pts)\n",
    "\n",
    "You can load the Fashion MNIST directly from Tensorflow. **Partition of the dataset** so that you will have 50,000 examples for training, 10,000 examples for validation, and 10,000 examples for testing. Also, make sure that you platten out each of examples so that it contains only a 1-D feature vector.\n",
    "\n",
    "Write some code to output the dimensionalities of each partition (train, validation, and test sets).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2-Ilkesfm7Z"
   },
   "outputs": [],
   "source": [
    "# Loading in data \n",
    "fmnist = tf.keras.datasets.fashion_mnist.load_data();\n",
    "\n",
    "# Creating train and test data sets \n",
    "(X_train, y_train), (X_test, y_test) = fmnist\n",
    "\n",
    "# Creating specific training, validation, and test sets \n",
    "x_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "x_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "x_valid, x_train = x_train[:10000], x_train[10000:]\n",
    "y_valid, y_train = y_train[:10000], y_train[10000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(X_train[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Na4CpxLBAGGP"
   },
   "source": [
    "- - -\n",
    "## 2. CONSTRUCTION PHASE (30 pts)\n",
    "\n",
    "In this section, define at least three neural networks with different structures. Make sure that the input layer has the right number of inputs. The best structure often is found through a process of trial and error experimentation:\n",
    "- You may start with a fully connected network structure with two hidden layers.\n",
    "- You may try a few settings of the number of nodes in each layer.\n",
    "- You may try a few activation functions to see if they affect the performance.\n",
    "\n",
    "**Important Implementation Note:** For the purpose of learning Tensorflow, you must use low-level TensorFlow API to construct the network. Usage of high-level tools (ie. Keras) is not permited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIJrHPVlAGGQ"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "reset_graph()\n",
    "\n",
    "# Set some configuration here\n",
    "n_inputs = 28*28  # Fashion-MNIST\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Construct placeholder for the input layer\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iDrFp7KKils6"
   },
   "outputs": [],
   "source": [
    "# n_hidden1 = 784\n",
    "# n_hidden2 = 196\n",
    "\n",
    "# with tf.name_scope(\"dnn1\"):\n",
    "#     hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "#     hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "#     logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luyNllyVkNxr"
   },
   "outputs": [],
   "source": [
    "# nhidden1 = 784\n",
    "# nhidden2 = 196\n",
    "# nhidden3 = 49\n",
    "\n",
    "# with tf.name_scope(\"dnn2\"):\n",
    "#     hidden_1 = tf.layers.dense(X, nhidden1, activation=leaky_relu, name=\"hidden_1\")\n",
    "#     hidden_2 = tf.layers.dense(hidden_1, nhidden2, activation=leaky_relu, name=\"hidden_2\")\n",
    "#     hidden_3 = tf.layers.dense(hidden_2, nhidden3, activation=leaky_relu, name=\"hidden_3\")\n",
    "#     logits_2 = tf.layers.dense(hidden_3, n_outputs, name=\"outputs_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JuvjZeJekP4-"
   },
   "outputs": [],
   "source": [
    "# BEST NETWORK \n",
    "\n",
    "\n",
    "# nhidden1 = 784\n",
    "# nhidden2 = 392\n",
    "# nhidden3 = 196\n",
    "# nhidden4 = 98\n",
    "\n",
    "\n",
    "# with tf.name_scope(\"dnn3\"):\n",
    "#     hidden_one = tf.layers.dense(X, nhidden1, activation=leaky_relu, name=\"hidden_one\")\n",
    "#     hidden_two = tf.layers.dense(hidden_one, nhidden2, activation=leaky_relu, name=\"hidden_two\")\n",
    "#     hidden_three = tf.layers.dense(hidden_two, nhidden3, activation=leaky_relu, name=\"hidden_three\")\n",
    "#     hidden_four = tf.layers.dense(hidden_three, nhidden4, activation=leaky_relu, name=\"hidden_four\")\n",
    "#     logits_3 = tf.layers.dense(hidden_three, n_outputs, name=\"outputs_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he_init = tf.variance_scaling_initializer()\n",
    "# training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "# dropout_rate = 0.5  # == 1 - keep_prob\n",
    "# X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "# nhidden1 = 784\n",
    "# nhidden2 = 392\n",
    "# nhidden3 = 196\n",
    "# nhidden4 = 98\n",
    "\n",
    "# with tf.name_scope(\"dnnBenchmark\"):\n",
    "#     my_batch_norm_layer = partial(\n",
    "#             tf.layers.batch_normalization,\n",
    "#             training=training,\n",
    "#             momentum=0.9)\n",
    "\n",
    "#     my_dense_layer = partial(\n",
    "#             tf.layers.dense,\n",
    "#             kernel_initializer=he_init)\n",
    "\n",
    "#     hidden1 = my_dense_layer(X_drop, nhidden1, name=\"hidden1\")\n",
    "#     bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "#     hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "#     hidden2 = my_dense_layer(hidden1_drop, nhidden2, name=\"hidden2\")\n",
    "#     bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "#     hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "#     hidden3 = my_dense_layer(hidden2_drop, nhidden3, name=\"hidden3\")\n",
    "#     bn3 = tf.nn.elu(my_batch_norm_layer(hidden3))\n",
    "#     hidden3_drop = tf.layers.dropout(hidden3, dropout_rate, training=training)\n",
    "#     hidden4 = my_dense_layer(hidden3_drop, nhidden4, name=\"hidden4\")\n",
    "#     bn4 = tf.nn.elu(my_batch_norm_layer(hidden4))\n",
    "#     hidden4_drop = tf.layers.dropout(hidden4, dropout_rate, training=training)\n",
    "#     logits_before_bn = my_dense_layer(hidden4_drop, n_outputs, name=\"outputs\")\n",
    "#     logits = my_batch_norm_layer(logits_before_bn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.variance_scaling_initializer()\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "# dropout_rate = 0.5  # == 1 - keep_prob\n",
    "# X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "nhidden1 = 784\n",
    "nhidden2 = 392\n",
    "nhidden3 = 196\n",
    "nhidden4 = 98\n",
    "\n",
    "with tf.name_scope(\"dnnBenchmark\"):\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=0.9)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, nhidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, nhidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    hidden3 = my_dense_layer(bn2, nhidden3, name=\"hidden3\")\n",
    "    bn3 = tf.nn.elu(my_batch_norm_layer(hidden3))\n",
    "    hidden4 = my_dense_layer(bn3, nhidden4, name=\"hidden4\")\n",
    "    bn4 = tf.nn.elu(my_batch_norm_layer(hidden4))\n",
    "    logits_before_bn = my_dense_layer(bn4, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mQCboA8ijWK"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy1 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy1, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope(\"loss2\"):\n",
    "#     xentropy2 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits_2)\n",
    "#     loss2 = tf.reduce_mean(xentropy2, name=\"loss2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope(\"loss3\"):\n",
    "#     xentropy3 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits_3)\n",
    "#     loss3 = tf.reduce_mean(xentropy3, name=\"loss3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GL_cXX09ih12"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"eval/Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKcVSGXOAGGT"
   },
   "source": [
    "- - -\n",
    "## 3. EXECUTION PHASE (30 pts)\n",
    "\n",
    "After you construct the three models of neural networks, you can compute the performance measure as the class accuracy. You will need to define the number of epochs and size of the training batch. You also might need to reset the graph each time your try a different model. To save time and avoid retraining, you should save the trained model and load it from disk to evaluate a test set. Pick the best model and answer the following:\n",
    "- Which model yields the best performance measure for your dataset? Provide a reason why it yields the best performance.\n",
    "- Why did you pick this many hidden layers?\n",
    "- Provide some justifiable reasons for selecting the number of neurons per hidden layers. \n",
    "- Which activation functions did you use?\n",
    "\n",
    "In the next session you will get a chance to finetune it further .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGDKdeZzAGGV"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "# shuffle_batch() shuffle the examples in a batch before training\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.94 Validation accuracy: 0.8259\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.8657\n",
      "10 Batch accuracy: 0.88 Validation accuracy: 0.875\n",
      "15 Batch accuracy: 0.96 Validation accuracy: 0.889\n",
      "20 Batch accuracy: 0.96 Validation accuracy: 0.8837\n",
      "25 Batch accuracy: 0.86 Validation accuracy: 0.8846\n",
      "30 Batch accuracy: 0.88 Validation accuracy: 0.8354\n",
      "35 Batch accuracy: 0.86 Validation accuracy: 0.8956\n",
      "Final test accuracy: 81.34%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(x_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: x_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "    saver.save(sess, \"./my_dnn_model.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: x_test, y: y_test})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Several different combinations of hidden layers and activation types were tested. There was some discrepencies in the number of neurons per layer, but I kept that fairly constant throughout all three deep neural networks. \n",
    "\n",
    "DNN #1: Tested 2 layers with RELU, ELU, and Leaky ELU activations with 784 in the first hidden layer and 196 in the second hidden layer. I picked these numbers arbitrarily with 784 being the number of inputs (28 * 28). I thought that was a good place to start with the total number of neurons. RELU activation yielded the highest accuracy of 88.08%.\n",
    "\n",
    "DNN #2: Tested 3 layers with RELU, ELU, and Leaky ELU activations with 784 in the first hidden layer, 196 in the second hidden layer, and 49 in the third hidden layer. These numbers are just factors of 784, but once again just chosen arbitrarily. Once again RELU activation yielded the highest accuracy of 88.70%. \n",
    "\n",
    "DNN #3: Tested 4 layers with RELU, ELU, and Leaky ELU activations with 784 in the first hidden layer, 392 in the second hidden layer, 196 in the third hidden layer, and 98 in the fourth hidden layer. Leaky RELU activation yielded the highest accuracy of 88.85%. This is the best model thus far, and will be tweaked in part four. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snnavcm0urN8"
   },
   "outputs": [],
   "source": [
    "# DNN1: 88.08%\n",
    "# DNN2: 88.70%\n",
    "# DNN3: 88.85%\n",
    "\n",
    "# Best final accuracy is for DNN 2: 88.85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-s2zv1SrAGGY"
   },
   "source": [
    "- - -\n",
    "## 4. FINETUNING THE NETWORK (25 pts)\n",
    "\n",
    "The best performance on the Fashion MNIST of a non-neural-net classifier is the Support Vector Classifier {\"C\":10,\"kernel\":\"poly\"} with 0.897 accuracy. In this section, you will see how close you can get to that accuracy, or (better yet) beat it! You will be able to see the performance of other ML methods below:\n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com\n",
    "\n",
    "Use the best model from the previous section and see if you can improve it further. To improve the performance of your model, You must make some modifications based upon the practical guidelines discuss in class. Here are a few decisions about the recommended network configurations you have to make:\n",
    "1. Initialization: Use He Initialization for your model\n",
    "2. Activation: Add ELU as the activation function throughout your hidden layers\n",
    "3. Normalization: Incorporate the batch normalization at every layer\n",
    "4. Regularization: Configure the dropout policy at 50% rate\n",
    "5. Optimization: Change Gradient Descent into Adam Optimization\n",
    "6. Your choice: make any other changes in 1-5 you deem necessary\n",
    "\n",
    "Keep in mind that the execution phase is essentially the same, so you can just run it from the above. See how much you gain in classification accuracy. Provide some justifications for the gain in performance. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hjDXZ5ws6Zpx"
   },
   "source": [
    "## Analysis \n",
    "\n",
    "The way I executed finetuning the network was through trial and error. I tested each new network configuration by adding it to the previous configurations. If the final accuracy went down, then other combinations of the network configurations were tested. \n",
    "\n",
    "Adding ELU activation brought down the accuracy a little, so I tested all following network configurations with all three types of activations: ELU, RELU, and Leaky RELU. \n",
    "\n",
    "Batch normalization really helped the accuracy, while regularization brought the overall accuracy down by a little. Adding Adam Optimization too brought the accuracy down by a lot. I took out regularization to just test batch normalization and Adam Optimization, and was met with a little higher accuracy than previous runs. \n",
    "\n",
    "I found that the best deep neural network had the configurations of Initialization + RELU Activation + Batch Normalization, because it yielded an accuracy of 88.91%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialization: 88.38%\n",
    "# 2. Initialization + ELU Activation: 87.09%\n",
    "\n",
    "# 3a. Initialization + ELU Activation + Batch Normalization: 87.83%\n",
    "# 3b. Initialization + RELU Activation + Batch Normalization: 88.91%\n",
    "# 3c. Initialization + Leaky Activation + Batch Normalization: 88.57%\n",
    "\n",
    "# 4a. Initialization + ELU Activation + Batch Normalization + Regularization: 83.80%\n",
    "# 4b. Initialization + RELU Activation + Batch Normalization + Regularization: 83.80%\n",
    "# 4c. Initialization + Leaky Activation + Batch Normalization + Regularization: 83.66%\n",
    "\n",
    "# 5a. Initialization + ELU Activation + Batch Normalization + Regularization + Adam Optimization: 79.40%\n",
    "# 5b. Initialization + RELU Activation + Batch Normalization + Regularization + Adam Optimization: 79.40%\n",
    "# 5c. Initialization + Leaky Activation + Batch Normalization + Regularization + Adam Optimization: 73.60%\n",
    "\n",
    "# 5c. Initialization + ELU Activation + Batch Normalization + Adam Optimization: 81.34%\n",
    "# 5d. Initialization + RELU Activation + Batch Normalization + Adam Optimization: 86.62%\n",
    "# 5e. Initialization + Leaky Activation + Batch Normalization + Adam Optimization: 87.62%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04jsbI9TAGGY"
   },
   "source": [
    "- - -\n",
    "## 5. OUTLOOK (5 pts)\n",
    "\n",
    "Plan for the outlook of your system: This may lead to the direction of your future project:\n",
    "- Did your neural network outperform other \"traditional ML technique? Why/why not?\n",
    "    - My neural network did not outperform other \"traditional ML techniques\" as the Support Vector Classifier had an accuracy of 89.2%. My best neural network model had an overall accuracy of 88.91%, which is fairly close to the best accuracy but still falls short. I feel as though neural networks are so complicated, and can be changed in so many different ways to yield different models. Obviously, for the scope of this assignment and with the time given, I was not able to change and test variations of all the different combinations of variables and parameters. \n",
    "\n",
    "- Does your model work well? If not, which model should be further investigated?\n",
    "    - My model works very well. My best tweaked model had an overall accuracy of 88.91%. I would love to further investigate this model by using different combinations of variables from the number of neurons per hidden layer to changing the batch momentum factor. Since training and testing each model takes so much time, it would have been too tedious to implement each individual change with all the different combinations of other parameters. If I had more time, I would definitely try to investigate my third neural network with RELU activation, He intialization, and Batch normalization. \n",
    "    \n",
    "- Do you satisfy with your system? What do you think needed to improve?\n",
    "    - I am not satisfied with my system. I think the notion of creating 3 arbitrary neural networks, and changing all the different parameters is a tedious job. It takes forever to test and train each changing neural network, and then with each added change the process starts over. I think the way to truly train the best neural network is to create a deep neural network through Grid Search, which will yield the best values for all the parameters in a neural network. That would allow us to test so many different combinations of variables, without having to manually change values and press run every single time. Grid Search would give us the best parameter values, and then we would just create our deep neural network off those values. This would be ideal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zS9PKaL4AGGZ"
   },
   "source": [
    "- - - \n",
    "### NEED HELP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0vuIEBDAGGa"
   },
   "source": [
    "In case you get stuck in any step in the process, you may find some useful information from:\n",
    "\n",
    " * Consult my lectures and/or the textbook\n",
    " * Talk to the TA, they are available and there to help you during OH\n",
    " * Come talk to me or email me <nn4pj@virginia.edu> with subject starting \"CS4501 Assignment 4:...\".\n",
    " * More on the Fashion-MNIST to be found here: https://hanxiao.github.io/2018/09/28/Fashion-MNIST-Year-In-Review/\n",
    "\n",
    "Best of luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cH_mulWEAGGb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS 4501 Assignment 4.ipynb",
   "provenance": [
    {
     "file_id": "1hQZ4t2l5aFDO0sEs213HsV547c_tH6TL",
     "timestamp": 1554445243544
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
